# utils_chatgpt.py
# Last modified: 2025-11-05 by Andr√©s Berm√∫dez

from openai import OpenAI
import logging
from typing import Any, Optional, Tuple, Dict
import os
import json
from utils import send_text_response, limpiar_respuesta_json, log_message

def get_openai_key() -> str:
    try:
        """Obtiene la clave API de OpenAI desde variables de entorno."""
        log_message('Iniciando funci√≥n <GetOpenAIKey>.', 'INFO')
        logging.info('Obteniendo clave de OpenAI')
        api_key: Optional[str] = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("No se encontr√≥ la clave OPENAI_API_KEY en las variables de entorno.")
        logging.info('Clave de OpenAI obtenida')
        log_message('Finalizando funci√≥n <GetOpenAIKey>.', 'INFO')
        return api_key
    except Exception as e:
        log_message(f"Error al obtener la clave de OpenAI: {e}", 'ERROR')
        logging.error(f"Error al obtener la clave de OpenAI: {e}")
        raise
    
def get_classifier(msj: str, sender: str) -> Tuple[Optional[str], Optional[str], Dict[str, Any]]:
    try:
        """Clasifica un mensaje de WhatsApp usando un modelo fine-tuned de OpenAI."""
        log_message('Iniciando funci√≥n <GetClassifier>.', 'INFO')
        logging.info('Clasificando mensaje')
        classificationPrompt: str = """
        Eres un clasificador de mensajes para un asistente de WhatsApp de un restaurante.
        Responde **√∫nicamente** en formato JSON v√°lido con la siguiente estructura:
        {
          "intent": "<la intenci√≥n detectada>",
          "type": "<el tipo de mensaje>",
          "entities": { }
        }
        No incluyas explicaciones ni texto fuera del JSON.
        """
        messages = [
            {"role": "system", "content": classificationPrompt},
            {"role": "user", "content": msj}
        ]
        client: OpenAI = OpenAI(api_key=get_openai_key())
        respuesta: Any = client.chat.completions.create(
            model="ft:gpt-3.5-turbo-0125:net-applications:domicilios:CJRSZZ2S",
            messages=messages,
            max_tokens=500,
            temperature=0.1
        )
        raw_response: str = respuesta.choices[0].message.content.strip()
        logging.info(f"[Clasificador RAW] {raw_response!r}")
        json_str: str = limpiar_respuesta_json(raw_response)
        result: Dict[str, Any] = json.loads(json_str)
        intent: Optional[str] = result.get("intent")
        type_: Optional[str] = result.get("type")
        entities: Dict[str, Any] = result.get("entities", {})
        if not intent or not type_:
            raise ValueError(f"Respuesta inv√°lida, faltan claves: {result}")
        logging.info(f"Respuesta del clasificador: {result}")
        logging.info(f"Intent: {intent}, Type: {type_}, Entities: {entities}")
        log_message('Finalizando funci√≥n <GetClassifier>.', 'INFO')
        return intent, type_, entities
    except Exception as e:
        log_message(f'Error al hacer uso de funci√≥n <GetClassifier>: {e}.', 'ERROR')
        logging.error(f"Error al clasificar el mensaje: {e}")
        send_text_response(sender, "Lo siento, hubo un error al procesar tu mensaje. ¬øPodr√≠as repetirlo?")
        return None, None, {}

def analizar_respuesta_usuario_sin_intencion(respuesta_cliente: str, intencion_anterior: str, model: str = "gpt-3.5-turbo") -> dict:
    """
    Analiza si la respuesta del usuario tiene continuidad con la intenci√≥n anterior.
    
    Par√°metros:
        respuesta_cliente (str): Respuesta dada por el usuario (e.g., "s√≠", "no", "no entiendo").
        intencion_anterior (str): Intenci√≥n previa detectada en el flujo conversacional.
        model (str): Modelo de OpenAI a utilizar.
    
    Retorna:
        dict: JSON con la estructura solicitada.
    """
    client: OpenAI = OpenAI()
    prompt = f"""
        Eres un asistente que analiza si la respuesta de un usuario mantiene continuidad con la intenci√≥n anterior.
        Tu salida debe ser un JSON EXACTO con la forma:
        {{
        "intencion_respuesta": "nombre_intencion",
        "continuidad": bool,
        "observaciones": "texto opcional si hay duda o ruptura de flujo"
        }}

        Ejemplos:

        1Ô∏è‚É£
        intencion_anterior: "consulta_promociones"
        respuesta_cliente: "s√≠"
        ‚Üí
        {{
        "intencion_respuesta": "consulta_promociones",
        "continuidad": true,
        "observaciones": ""
        }}

        2Ô∏è‚É£
        intencion_anterior: "consulta_promociones"
        respuesta_cliente: "no"
        ‚Üí
        {{
        "intencion_respuesta": "consulta_promociones",
        "continuidad": false,
        "observaciones": "El usuario rechaz√≥ la continuidad de la promoci√≥n"
        }}

        3Ô∏è‚É£
        intencion_anterior: "consulta_productos"
        respuesta_cliente: "no entiendo"
        ‚Üí
        {{
        "intencion_respuesta": "consulta_productos",
        "continuidad": false,
        "observaciones": "El usuario parece confundido o necesita aclaraci√≥n"
        }}

        4Ô∏è‚É£
        intencion_anterior: "reclamo_servicio"
        respuesta_cliente: "ya lo solucionaron"
        ‚Üí
        {{
        "intencion_respuesta": "reclamo_servicio",
        "continuidad": false,
        "observaciones": "El usuario indica que el problema ya se resolvi√≥"
        }}

        5Ô∏è‚É£
        intencion_anterior: "consulta_horarios"
        respuesta_cliente: "perfecto, gracias"
        ‚Üí
        {{
        "intencion_respuesta": "consulta_horarios",
        "continuidad": true,
        "observaciones": "El usuario confirma que recibi√≥ la informaci√≥n"
        }}

        Ahora analiza:
        intencion_anterior: "{intencion_anterior}"
        respuesta_cliente: "{respuesta_cliente}"

        Devuelve SOLO el JSON, sin explicaci√≥n.
    """ 
    response = client.responses.create(
        model=model,
        input=prompt,
        temperature=0.3
    )
    text_output = response.output[0].content[0].text.strip()
    try:
        result = json.loads(text_output)
    except json.JSONDecodeError:
        result = {
            "intencion_respuesta": intencion_anterior,
            "continuidad": False,
            "observaciones": "Error al interpretar la respuesta del modelo"
        }
    log_message('Finalizando funci√≥n <AnalizarRespuestaUsuarioSinIntencion>.', 'INFO')
    log_message(f'Respuesta analizada: {result}', 'INFO')
    return result

def clasificar_pregunta_menu_chatgpt(pregunta_usuario: str, model: str = "gpt-3.5-turbo") -> dict:
    """
    Clasifica si una pregunta del usuario est√° relacionada con el men√∫ de una hamburgueser√≠a
    usando un modelo de lenguaje (ChatGPT).
    
    Par√°metros:
        pregunta_usuario (str): Pregunta escrita por el usuario.
        model (str): Modelo de OpenAI a utilizar.
    
    Retorna:
        dict: JSON con la estructura solicitada.
    """
    log_message('Iniciando funci√≥n <ClasificarPreguntaMenuChatGPT>.', 'INFO')
    client: OpenAI = OpenAI()
    prompt: str = f"""
    Eres un asistente que clasifica preguntas de clientes de una hamburgueser√≠a.
    Debes responder con un JSON EXACTO con la siguiente forma:
    {{
        "clasificacion": "relacionada" o "no_relacionada"
    }}
    Instrucciones:
    - Si la pregunta se refiere a comidas, hamburguesas, bebidas, malteadas, ingredientes, precios, combos, 
      opciones vegetarianas o cualquier cosa del men√∫ de una hamburgueser√≠a ‚Üí "relacionada".
    - Si la pregunta es sobre temas generales, ajenos al restaurante (por ejemplo: Bogot√°, Python, clima, cielo, etc.) ‚Üí "no_relacionada".
    Ejemplos:
    1Ô∏è‚É£ "qu√© hamburguesas tienen?" ‚Üí {{"clasificacion": "relacionada"}}
    2Ô∏è‚É£ "hay hamburguesas de pollo?" ‚Üí {{"clasificacion": "relacionada"}}
    3Ô∏è‚É£ "qu√© malteadas tienen?" ‚Üí {{"clasificacion": "relacionada"}}
    4Ô∏è‚É£ "tienen opciones vegetarianas?" ‚Üí {{"clasificacion": "relacionada"}}
    5Ô∏è‚É£ "d√≥nde queda Bogot√°?" ‚Üí {{"clasificacion": "no_relacionada"}}
    6Ô∏è‚É£ "qu√© es Python?" ‚Üí {{"clasificacion": "no_relacionada"}}
    7Ô∏è‚É£ "por qu√© el cielo es azul?" ‚Üí {{"clasificacion": "no_relacionada"}}
    Ahora clasifica la siguiente pregunta del usuario:
    "{pregunta_usuario}"
    Devuelve SOLO el JSON, sin explicaci√≥n adicional.
    """
    try:
        response = client.responses.create(
            model=model,
            input=prompt,
            temperature=0
        )
        text_output = response.output[0].content[0].text.strip()
        result = json.loads(text_output)
        log_message('Finalizando funci√≥n <ClasificarPreguntaMenuChatGPT>.', 'INFO')
        return result
    except json.JSONDecodeError:
        logging.error(f"Error al parsear JSON: {text_output}")
        log_message(f'Error al parsear JSON en <ClasificarPreguntaMenuChatGPT>: {text_output}', 'ERROR')
        return {"clasificacion": "no_relacionada"}
    except Exception as e:
        logging.error(f"Error en <ClasificarPreguntaMenuChatGPT>: {e}")
        log_message(f'Error en <ClasificarPreguntaMenuChatGPT>: {e}.', 'ERROR')
        return {"clasificacion": "no_relacionada"}

def responder_pregunta_menu_chatgpt(pregunta_usuario: str, items, model: str = "gpt-4o-mini") -> dict:
    """
    Responde preguntas del usuario sobre el men√∫ de Sierra Nevada üçî,
    con base en los √≠tems reales disponibles.
    
    Retorna:
        {
            "respuesta": str,
            "recomendacion": bool,
            "productos": list[str]
        }
    """
    log_message('Iniciando funci√≥n <ResponderPreguntaMenuChatGPT>.', 'INFO')

    # üîç 1Ô∏è‚É£ Normalizamos texto
    pregunta_lower = pregunta_usuario.lower()
    coincidencias = []

    for item in items:
        texto_busqueda = f"{item.get('nombre', '')} {item.get('descripcion', '')}".lower()
        if any(palabra in texto_busqueda for palabra in pregunta_lower.split()):
            coincidencias.append(item)

    # üîç 2Ô∏è‚É£ Generamos el prompt
    if coincidencias:
        prompt = f"""
        Eres un asistente amable de la hamburgueser√≠a "Sierra Nevada" en Bogot√° üçî.
        El cliente pregunt√≥: "{pregunta_usuario}"

        Estos √≠tems del men√∫ coinciden con la b√∫squeda:
        {json.dumps(coincidencias, ensure_ascii=False)}

        Instrucciones:
        - Usa solo los productos listados.
        - No inventes nombres ni ingredientes.
        - Responde con un tono natural y cercano, tipo WhatsApp.
        - Devuelve SOLO un JSON con este formato exacto:
        {{
            "respuesta": "texto amigable para el cliente",
            "recomendacion": false,
            "productos": ["nombre1", "nombre2"]
        }}
        """
    else:
        prompt = f"""
        Eres un asistente amable de la hamburgueser√≠a "Sierra Nevada" en Bogot√° üçî.
        El cliente pregunt√≥: "{pregunta_usuario}"

        Aqu√≠ tienes el men√∫ completo:
        {json.dumps(items, ensure_ascii=False)}

        Instrucciones:
        - No hay coincidencias exactas con lo que pregunta el cliente.
        - No inventes productos.
        - Di con amabilidad que no tenemos eso, pero recomienda hasta 2 √≠tems similares del men√∫.
        - Usa los nombres reales en 'items'.
        - Devuelve SOLO un JSON con este formato exacto:
        {{
            "respuesta": "texto amigable para el cliente",
            "recomendacion": true,
            "productos": ["nombre1", "nombre2"]
        }}
        Ejemplo:
        Usuario: "¬øTienen hamburguesas de pollo?"
        Si no hay pollo: -> {{"respuesta": "No tenemos hamburguesas de pollo üòî, pero te puedo recomendar la Sierra Coste√±a o la Cl√°sica üçî", "recomendacion": true, "productos": ["Sierra Coste√±a", "Cl√°sica"]}}
        """

    # ‚öôÔ∏è 3Ô∏è‚É£ Llamada al modelo
    try:
        client = OpenAI()
        response = client.responses.create(
            model=model,
            input=prompt,
            temperature=0.5
        )

        text_output = response.output[0].content[0].text.strip()

        try:
            result = json.loads(text_output)
        except json.JSONDecodeError:
            logging.error(f"Error al parsear JSON: {text_output}")
            log_message(f'Error al parsear JSON en <ResponderPreguntaMenuChatGPT>: {text_output}', 'ERROR')
            result = {
                "respuesta": text_output,
                "recomendacion": False,
                "productos": []
            }

        # üßπ Validaci√≥n: asegurar campos siempre presentes
        if "productos" not in result:
            result["productos"] = []
        if "recomendacion" not in result:
            result["recomendacion"] = False

        log_message('Finalizando funci√≥n <ResponderPreguntaMenuChatGPT>.', 'INFO')
        return result

    except Exception as e:
        logging.error(f"Error en <ResponderPreguntaMenuChatGPT>: {e}")
        log_message(f'Error en <ResponderPreguntaMenuChatGPT>: {e}.', 'ERROR')
        return {
            "respuesta": "Lo siento, tuve un problema para responder üòî.",
            "recomendacion": False,
            "productos": []
        }
